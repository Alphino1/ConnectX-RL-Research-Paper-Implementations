{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17592,"databundleVersionId":899221,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rahuljaisy/alphazero-self-play-reinforcement-agent?scriptVersionId=247925302\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# AlphaZero for ConnectX: Implementation\n\n## Introduction: AlphaZero Methodology\n\nAlphaZero is a groundbreaking reinforcement learning algorithm developed by DeepMind that achieves superhuman performance in complex games through pure self-play learning. Unlike traditional game AI approaches that rely on handcrafted heuristics or expert data, AlphaZero learns tabula rasa: starting from random play with no domain knowledge beyond the game rules.\n\nThis implementation faithfully replicates the core methodology described in the 2017 paper:\n\"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\"  \nhttps://arxiv.org/abs/1712.01815\n\n---\n\n## Core Components of AlphaZero\n\n### 1. Monte Carlo Tree Search (MCTS) with Neural Network Guidance\n\n**PUCT Selection Formula**  \nAlphaZero balances exploration and exploitation using the PUCT formula:\n\n$$\nU(s, a) = c_{\\text{puct}} \\cdot P(s, a) \\cdot \\frac{\\sqrt{\\sum_b N(s, b)}}{1 + N(s, a)}\n$$\n\nWhere:  \n- c_puct is the exploration constant  \n- P(s, a) is the prior probability from the policy network  \n- N(s, a) is the visit count for action a at state s  \n- sum_b N(s, b) is the total number of visits to all child actions b from state s\n\n**Root Node Exploration**  \nTo encourage diverse exploration at the start of games, Dirichlet noise is added to the root node prior probabilities:  \n- epsilon = 0.25  \n- alpha = 0.3\n\n**Backpropagation**  \nDuring MCTS, value estimates are propagated back up the tree with alternating signs to reflect which player the value is for. This ensures symmetry between players during self-play.\n\n---\n\n### 2. Residual Convolutional Network\n\n**Network Architecture**  \nThe neural network uses a shared convolutional backbone with two output heads:  \n- Policy head: outputs a probability distribution over legal actions  \n- Value head: outputs an estimate of the expected outcome of the position\n\n**Residual Blocks**  \nThe network consists of 5 residual blocks with 128 filters each. Batch normalization follows each convolution to ensure stable learning.\n\n**Input Representation**  \nEach game state is encoded as a two-channel binary tensor:  \n- Channel 1: positions of the current player's pieces  \n- Channel 2: positions of the opponent's pieces\n\nThis representation preserves the turn-based nature of the game.\n\n---\n\n### 3. Self-Play Training Paradigm\n\n**Data Generation**  \nThe agent generates its own training data by playing games against itself using MCTS-guided moves. Each move contributes a training sample of the form:\n\n    (state, policy_target, value_target)\n\n- policy_target is derived from the visit counts of MCTS at that state  \n- value_target is the final game result, recorded from the perspective of the player who made the move\n\n**Loss Function**  \nThe neural network is trained using the following combined loss function:\n\n $Loss = (z - v)^2 - \\pi^T \\log(p) + c \\cdot \\|\\theta\\|^2$\n\n\nWhere:  \n- z is the true outcome of the game  \n- v is the predicted value  \n- pi is the target policy distribution  \n- p is the predicted policy  \n- theta are the network weights  \n- c is the regularization constant (L2 penalty)\n\n---\n\n## ConnectX Adaptation\n\nThis implementation adapts AlphaZero to the ConnectX environment on Kaggle. ConnectX is a 6x7 grid-based game where two players take turns dropping discs to align 4 in a row horizontally, vertically, or diagonally.\n\n**Key Modifications for ConnectX**  \n- Input state is represented as a 2-channel board tensor  \n- A fast win-check function scans 4 directions around the last move  \n- MCTS is run with 200 simulations per move (AlphaZero used 1600)  \n- Each training iteration consists of 50 self-play games (AlphaZero used thousands)\n\n---\n\n## Research Validation\n\nThe design and training pipeline in this project tries to maintain fidelity to AlphaZeroâ€™s original architecture:\n\n1. Learning is based entirely on self-play without expert data  \n2. A deep residual convolutional network predicts both move probabilities and board value  \n3. Dirichlet noise is applied at the root of the MCTS to enable exploration  \n4. MCTS uses the PUCT formula to balance prior knowledge and visit counts  \n5. Network training uses targets derived from MCTS search statistics\n\n---\n\n## Objective\n\nThis work tries to demonstrate how the AlphaZero algorithm can be adapted to ConnectX, a competitive environment with practical constraints. Despite reduced simulations and training scale, the method tries to remain structurally faithful to DeepMind's original implementation and offers a baseline implementation for reinforcement learning research via self-play.","metadata":{"_uuid":"53375b24-b6cc-4586-9ed9-10d02d64e11f","_cell_guid":"f5b3293e-34ce-413a-b19b-47bb187c1d16","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Import statements and library dependencies","metadata":{"_uuid":"85f341eb-dc1b-485d-bbd8-63981e45899e","_cell_guid":"74c88a38-6835-40d9-bfbf-b189d166fa01","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport random\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom collections import deque\nimport base64\nimport io\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"71c9111f-8b9c-4849-bc5c-15693e5f3fe0","_cell_guid":"9a23d339-e621-4aca-a399-00e4a08e51a1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Game configuration: DeepMind's approach for board games","metadata":{"_uuid":"ccafdbdf-de5d-4c25-84d6-e29ec5290a71","_cell_guid":"1ab2de3e-54fa-4df6-930a-59620562d1ec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"ROWS, COLS, IN_A_ROW = 6, 7, 4\nACTION_SIZE = COLS","metadata":{"_uuid":"178af8a5-340e-4d81-96ac-4ea4579485bd","_cell_guid":"64596753-2070-447a-a853-c2b94928a5fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## State class for ConnectX with optimized win detection","metadata":{"_uuid":"dee31786-b7d0-4c80-b354-ec2280d499bf","_cell_guid":"38a9d1eb-352c-4eea-b087-3b42aeb4c7f8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# AlphaZero state representation\nclass ConnectXState:\n    \"\"\"Game state with optimized win detection\"\"\"\n    def __init__(self):\n        self.board = np.zeros((ROWS, COLS), dtype=np.int8)\n        self.current_player = 1\n        self.winner = 0\n        self.terminal = False\n        self.last_move = (-1, -1)\n        \n    def clone(self):\n        st = ConnectXState()\n        st.board = self.board.copy()\n        st.current_player = self.current_player\n        st.winner = self.winner\n        st.terminal = self.terminal\n        st.last_move = self.last_move\n        return st\n        \n    def available_actions(self):\n        return [c for c in range(COLS) if self.board[0, c] == 0]\n        \n    def apply_action(self, action):\n        # Find first empty row in column\n        row = -1\n        for r in range(ROWS-1, -1, -1):\n            if self.board[r, action] == 0:\n                row = r\n                self.board[r, action] = self.current_player\n                self.last_move = (r, action)\n                break\n                \n        if row == -1:\n            raise ValueError(f\"Invalid action {action}\")\n            \n        # Check win condition around last move\n        if self._check_winner(row, action):\n            self.winner = self.current_player\n            self.terminal = True\n        elif np.all(self.board != 0):\n            self.terminal = True\n        else:\n            self.current_player = 3 - self.current_player\n\n    def _check_winner(self, row, col):\n        \"\"\"Efficient win detection in 4 directions (AlphaZero optimization)\"\"\"\n        player = self.board[row, col]\n        if player == 0:\n            return False\n        \n        # Directions: horizontal, vertical, diag down-right, diag up-right\n        directions = [(0, 1), (1, 0), (1, 1), (-1, 1)]\n        \n        for dr, dc in directions:\n            count = 1  # Current piece\n            \n            # Check in positive direction\n            r, c = row + dr, col + dc\n            while 0 <= r < ROWS and 0 <= c < COLS and self.board[r, c] == player:\n                count += 1\n                r += dr\n                c += dc\n                \n            # Check in negative direction\n            r, c = row - dr, col - dc\n            while 0 <= r < ROWS and 0 <= c < COLS and self.board[r, c] == player:\n                count += 1\n                r -= dr\n                c -= dc\n                \n            if count >= IN_A_ROW:\n                return True\n                \n        return False\n\n    def to_network_input(self):\n        \"\"\"AlphaZero's 2-channel input representation\"\"\"\n        board = self.board\n        player = self.current_player\n        channel0 = (board == player).astype(np.float32)\n        channel1 = (board == (3 - player)).astype(np.float32)\n        return np.stack([channel0, channel1])","metadata":{"_uuid":"76006a0c-6d43-4244-b099-299a00db0d1b","_cell_guid":"7368f6ff-8001-4ba0-a24a-696d4096cf4a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Neural Network Architecture (AlphaZero specification)","metadata":{"_uuid":"37221c14-c82d-43fc-bef0-ee0c0d771599","_cell_guid":"139e0147-e917-4410-ba43-07b0eb311beb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    \"\"\"Residual block as per DeepMind's implementation\"\"\"\n    def __init__(self, filters):\n        super().__init__()\n        self.conv1 = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(filters)\n        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(filters)\n        \n    def forward(self, x):\n        residual = x\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.bn2(self.conv2(x))\n        x += residual\n        return F.relu(x)\n\nclass AlphaZeroNet(nn.Module):\n    \"\"\"AlphaZero neural network with reduced capacity for faster training\"\"\"\n    def __init__(self, num_blocks=3, filters=64):\n        super().__init__()\n        # Input convolution\n        self.conv_input = nn.Conv2d(2, filters, kernel_size=3, padding=1)\n        self.bn_input = nn.BatchNorm2d(filters)\n        \n        # Residual tower (reduced from 5 to 3 blocks)\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(filters) for _ in range(num_blocks)]\n        )\n        \n        # Policy head\n        self.conv_policy = nn.Conv2d(filters, 2, kernel_size=1)\n        self.bn_policy = nn.BatchNorm2d(2)\n        self.fc_policy = nn.Linear(2 * ROWS * COLS, ACTION_SIZE)\n        \n        # Value head\n        self.conv_value = nn.Conv2d(filters, 1, kernel_size=1)\n        self.bn_value = nn.BatchNorm2d(1)\n        self.fc_value1 = nn.Linear(1 * ROWS * COLS, 128)\n        self.fc_value2 = nn.Linear(128, 1)\n        \n    def forward(self, x):\n        # Input convolution\n        x = F.relu(self.bn_input(self.conv_input(x)))\n        \n        # Residual tower\n        x = self.res_blocks(x)\n        \n        # Policy head\n        p = F.relu(self.bn_policy(self.conv_policy(x)))\n        p = p.reshape(p.size(0), -1)\n        p = self.fc_policy(p)\n        \n        # Value head\n        v = F.relu(self.bn_value(self.conv_value(x)))\n        v = v.reshape(v.size(0), -1)\n        v = F.relu(self.fc_value1(v))\n        v = torch.tanh(self.fc_value2(v))\n        \n        return p, v.squeeze(1)","metadata":{"_uuid":"01b1b7d4-6083-44d5-8ea8-1aed443e1691","_cell_guid":"08474cec-a77c-4ea7-b8f7-7df7d741c299","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Monte Carlo Tree Search (Implementation)","metadata":{"_uuid":"78e443cb-7c3b-40bd-8284-bd30760e4afb","_cell_guid":"f641eb60-e280-449e-b697-8849506fe877","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MCTSNode:\n    \"\"\"MCTS node with AlphaZero enhancements\"\"\"\n    __slots__ = ('state', 'parent', 'children', 'N', 'W', 'Q', 'P')\n    \n    def __init__(self, state, prior, parent=None):\n        self.state = state\n        self.parent = parent\n        self.children = {}\n        self.N = 0  # Visit count\n        self.W = 0.0  # Total action value\n        self.Q = 0.0  # Mean action value\n        self.P = prior  # Prior probability\n        \n    def is_leaf(self):\n        return len(self.children) == 0\n    \n    def is_root(self):\n        return self.parent is None\n\nclass MCTS:\n    \"\"\"Monte Carlo Tree Search with AlphaZero enhancements\"\"\"\n    def __init__(self, net, c_puct=1.25, dirichlet_alpha=0.3, dirichlet_epsilon=0.25):\n        self.net = net\n        self.c_puct = c_puct\n        self.dirichlet_alpha = dirichlet_alpha\n        self.dirichlet_epsilon = dirichlet_epsilon\n        \n    def search(self, root_state, num_simulations):\n        \"\"\"Execute MCTS from given state (AlphaZero pseudocode implementation)\"\"\"\n        root = MCTSNode(root_state, prior=0.0)\n        \n        # Run simulations\n        for _ in range(num_simulations):\n            node = root\n            search_path = [node]\n            \n            # SELECTION (PUCT formula)\n            while not node.is_leaf():\n                best_score = -float('inf')\n                best_action = None\n                \n                for action, child in node.children.items():\n                    # PUCT formula: Q + U\n                    u = self.c_puct * child.P * math.sqrt(node.N) / (1 + child.N)\n                    score = child.Q + u\n                    \n                    if score > best_score:\n                        best_score = score\n                        best_action = action\n                \n                node = node.children[best_action]\n                search_path.append(node)\n            \n            # EXPANSION\n            if not node.state.terminal:\n                # Add Dirichlet noise to root node (AlphaZero enhancement)\n                if node.is_root():\n                    policy, value = self.net_predict(node.state)\n                    policy = (1 - self.dirichlet_epsilon) * policy + \\\n                             self.dirichlet_epsilon * np.random.dirichlet(\n                                 [self.dirichlet_alpha] * len(policy))\n                else:\n                    policy, value = self.net_predict(node.state)\n                \n                # Expand node\n                for action in node.state.available_actions():\n                    new_state = node.state.clone()\n                    new_state.apply_action(action)\n                    node.children[action] = MCTSNode(new_state, policy[action], node)\n            else:\n                # Terminal state evaluation\n                if node.state.winner == 0:\n                    value = 0.0  # Draw\n                else:\n                    # Win/loss from perspective of current player at node\n                    value = 1.0 if node.state.winner == node.state.current_player else -1.0\n            \n            # BACKPROPAGATION (with alternating signs)\n            self._backpropagate(search_path, value)\n                \n        return root\n    \n    def _backpropagate(self, path, value):\n        \"\"\"Backpropagate value through path with alternating signs\"\"\"\n        for node in reversed(path):\n            node.N += 1\n            node.W += value\n            node.Q = node.W / node.N\n            value = -value  # Value from parent's perspective\n            \n    def net_predict(self, state):\n        \"\"\"Get policy and value predictions from network\"\"\"\n        x = state.to_network_input()[np.newaxis, ...]\n        x = torch.tensor(x, dtype=torch.float32, device=device)\n        \n        with torch.no_grad():\n            logits, value = self.net(x)\n        policy = F.softmax(logits, dim=1).cpu().numpy().flatten()\n        return policy, value.item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Self-play and training functions","metadata":{"_uuid":"45662eb3-17b6-4f27-ac5e-ec4e36fc8826","_cell_guid":"a5df4f66-8ea5-46f1-99f0-4b2a3ff956bc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Self-play and training functions\ndef self_play(net, num_games, num_simulations, temperature=1.0):\n    \"\"\"AlphaZero self-play data generation\"\"\"\n    data = []\n    mcts = MCTS(net)\n    \n    for _ in range(num_games):\n        state = ConnectXState()\n        history = []\n        \n        while not state.terminal:\n            root = mcts.search(state, num_simulations)\n            \n            # Compute improved policy from visit counts\n            counts = np.zeros(ACTION_SIZE)\n            for action, child in root.children.items():\n                counts[action] = child.N\n                \n            # Temperature-based policy (AlphaZero Eq. 1)\n            policy = counts ** (1/temperature)\n            policy /= policy.sum()\n            \n            history.append((state.clone(), policy, state.current_player))\n            \n            # Sample action from improved policy\n            action = np.random.choice(ACTION_SIZE, p=policy)\n            state.apply_action(action)\n        \n        # Assign outcome from each player's perspective\n        outcome = 0\n        if state.winner != 0:\n            outcome = 1 if state.winner == state.current_player else -1\n        \n        # Create training targets (perspective adjusted)\n        for i, (s, pi, player) in enumerate(history):\n            perspective = 1 if player == state.current_player else -1\n            z = outcome * perspective\n            data.append((s, pi, z))\n            \n    return data\n\ndef train_network(net, data, optimizer, batch_size=256, epochs=1):\n    \"\"\"AlphaZero training procedure with loss function (Eq. 3)\"\"\"\n    net.train()\n    states, policies, values = [], [], []\n    \n    # Prepare dataset\n    for state, pi, z in data:\n        states.append(state.to_network_input())\n        policies.append(pi)\n        values.append(z)\n    \n    # Convert to tensors\n    states = torch.tensor(np.array(states), dtype=torch.float32, device=device)\n    policies = torch.tensor(np.array(policies), dtype=torch.float32, device=device)\n    values = torch.tensor(np.array(values), dtype=torch.float32, device=device)\n    \n    # Create DataLoader for efficient batching\n    dataset = torch.utils.data.TensorDataset(states, policies, values)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    policy_losses, value_losses = [], []\n    \n    for _ in range(epochs):\n        for batch_states, batch_policies, batch_values in loader:\n            # Forward pass\n            logits, v_pred = net(batch_states)\n            \n            # Policy loss (cross-entropy)\n            policy_loss = -(batch_policies * F.log_softmax(logits, dim=1)).sum(dim=1).mean()\n            \n            # Value loss (MSE)\n            value_loss = F.mse_loss(v_pred, batch_values)\n            \n            # Combined loss (AlphaZero Eq. 3)\n            loss = policy_loss + value_loss\n            \n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n            optimizer.step()\n            \n            policy_losses.append(policy_loss.item())\n            value_losses.append(value_loss.item())\n    \n    return np.mean(policy_losses), np.mean(value_losses)","metadata":{"_uuid":"951bfdb6-f893-431d-9bd0-13cb2d191c00","_cell_guid":"714c6f09-02d7-4c0b-b0d4-d26b01cbf638","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training execution and visualization","metadata":{"_uuid":"4831723c-6fe7-4fd1-8f78-a2a273ac7584","_cell_guid":"fd17f3ae-1454-4498-a378-0d28a0b9b4da","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Training execution (Complete AlphaZero pipeline)\nif __name__ == '__main__':\n    # Device configuration\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Hyperparameters (optimized for 20 minute training)\n    NUM_ITERATIONS = 15\n    SELF_PLAY_GAMES = 30\n    MCTS_SIMULATIONS = 50\n    TRAINING_EPOCHS = 1\n    REPLAY_BUFFER_SIZE = 1000\n    TEMPERATURE_START = 1.0\n    TEMPERATURE_DECAY = 0.95\n    BATCH_SIZE = 256\n    LEARNING_RATE = 0.002\n    \n    # Initialize network\n    net = AlphaZeroNet(num_blocks=3, filters=64).to(device)\n    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n    \n    # Training metrics\n    policy_losses = []\n    value_losses = []\n    win_rates = []\n    replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n    temperature = TEMPERATURE_START\n    \n    # AlphaZero training loop\n    for iteration in range(NUM_ITERATIONS):\n        print(f\"\\n=== AlphaZero Iteration {iteration+1}/{NUM_ITERATIONS} ===\")\n        \n        # 1. Self-play data generation\n        print(f\"Generating {SELF_PLAY_GAMES} self-play games (temp={temperature:.2f})...\")\n        new_data = self_play(net, SELF_PLAY_GAMES, MCTS_SIMULATIONS, temperature)\n        replay_buffer.extend(new_data)\n        \n        # Decay temperature\n        temperature = max(0.1, temperature * TEMPERATURE_DECAY)\n        \n        # 2. Network training\n        print(f\"Training on {len(replay_buffer)} samples...\")\n        policy_loss, value_loss = train_network(\n            net, \n            list(replay_buffer), \n            optimizer,\n            batch_size=BATCH_SIZE,\n            epochs=TRAINING_EPOCHS\n        )\n        policy_losses.append(policy_loss)\n        value_losses.append(value_loss)\n        \n        # 3. Performance evaluation (against random agent)\n        print(\"Evaluating against random agent...\")\n        wins = 0\n        for game in range(20):\n            state = ConnectXState()\n            while not state.terminal:\n                if state.current_player == 1:  # AlphaZero agent\n                    root = MCTS(net).search(state, MCTS_SIMULATIONS)\n                    # Select action with highest visit count\n                    action = max(root.children.items(), key=lambda x: x[1].N)[0]\n                else:  # Random opponent\n                    action = np.random.choice(state.available_actions())\n                state.apply_action(action)\n            \n            if state.winner == 1:  # AlphaZero is player 1\n                wins += 1\n                \n        win_rate = wins / 20\n        win_rates.append(win_rate)\n        print(f\"Win rate: {win_rate:.2f} | Policy loss: {policy_loss:.4f} | Value loss: {value_loss:.4f}\")\n    \n    # Save final model\n    torch.save(net.state_dict(), \"trained_alphazero.pth\")\n    print(\"\\nAlphaZero training complete!\")\n\n# Training performance visualization\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(win_rates, 'o-')\n    plt.title('Win Rate vs Random')\n    plt.xlabel('Iteration')\n    plt.ylabel('Win Rate')\n    plt.grid(True)\n    plt.ylim(0, 1)\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(policy_losses, 'o-', label='Policy Loss')\n    plt.plot(value_losses, 'o-', label='Value Loss')\n    plt.title('Training Losses')\n    plt.xlabel('Iteration')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.subplot(1, 3, 3)\n    plt.plot(value_losses, 'o-', color='red')\n    plt.title('Value Loss Convergence')\n    plt.xlabel('Iteration')\n    plt.ylabel('Value Loss')\n    plt.yscale('log')\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('alphazero_training_metrics.png')\n    plt.show()\n    ","metadata":{"_uuid":"2267fa44-6436-402f-b17c-bfb8a5b7e9dc","_cell_guid":"28794e41-9358-4ffc-a00a-94a72162db07","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission preparation: save model weights and generate `submission.py`","metadata":{"_uuid":"ac8c6099-b61f-45aa-aac1-7c2ee7823ee3","_cell_guid":"a4ff48e3-e6d2-4647-9d78-15910c8c03ad","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Create submission agent with trained weights\nbuffer = io.BytesIO()\ntorch.save(net.state_dict(), buffer)\nbuffer.seek(0)\nMODEL_WEIGHTS = base64.b64encode(buffer.read()).decode('utf-8')\n\nsubmission_code = f\"\"\"\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport base64\nimport io\n\nROWS, COLS = 6, 7\n\nclass AlphaZeroNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Input convolution\n        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        # Residual blocks\n        self.res1 = self._residual_block(64)\n        self.res2 = self._residual_block(64)\n        self.res3 = self._residual_block(64)\n        \n        # Policy head\n        self.policy_conv = nn.Conv2d(64, 2, kernel_size=1)\n        self.policy_bn = nn.BatchNorm2d(2)\n        self.policy_fc = nn.Linear(2*ROWS*COLS, COLS)\n        \n        # Value head\n        self.value_conv = nn.Conv2d(64, 1, kernel_size=1)\n        self.value_bn = nn.BatchNorm2d(1)\n        self.value_fc1 = nn.Linear(1*ROWS*COLS, 64)\n        self.value_fc2 = nn.Linear(64, 1)\n\n    def _residual_block(self, filters):\n        return nn.Sequential(\n            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n            nn.BatchNorm2d(filters),\n            nn.ReLU(),\n            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n            nn.BatchNorm2d(filters)\n        )\n        \n    def forward(self, x):\n        # Feature extraction\n        x = F.relu(self.bn1(self.conv1(x)))\n        \n        # Residual blocks\n        identity = x\n        x = F.relu(self.res1(x) + identity\n        identity = x\n        x = F.relu(self.res2(x) + identity\n        identity = x\n        x = F.relu(self.res3(x) + identity\n        \n        # Policy head\n        p = F.relu(self.policy_bn(self.policy_conv(x)))\n        p = p.view(p.size(0), -1)\n        p = self.policy_fc(p)\n        \n        # Value head\n        v = F.relu(self.value_bn(self.value_conv(x)))\n        v = v.view(v.size(0), -1)\n        v = F.relu(self.value_fc1(v))\n        v = torch.tanh(self.value_fc2(v))\n        \n        return p, v.squeeze(1)\n\n# Model loader with caching\n_model = None\n\ndef load_model():\n    global _model\n    if _model is None:\n        _model = AlphaZeroNet()\n        weights = base64.b64decode(\"{MODEL_WEIGHTS}\")\n        buffer = io.BytesIO(weights)\n        _model.load_state_dict(torch.load(buffer, map_location=torch.device('cpu')))\n        _model.eval()\n    return _model\n\ndef agent(observation, configuration):\n    # Get legal moves\n    board = np.array(observation['board'])\n    legal_moves = [c for c in range(COLS) if board[c] == 0]\n    \n    try:\n        model = load_model()\n        board_tensor = board.reshape(ROWS, COLS).astype(np.float32)\n        mark = observation['mark']\n        \n        # Create input tensor\n        if mark == 1:\n            cur = (board_tensor == 1).astype(np.float32)\n            opp = (board_tensor == 2).astype(np.float32)\n        else:\n            cur = (board_tensor == 2).astype(np.float32)\n            opp = (board_tensor == 1).astype(np.float32)\n        \n        x = np.stack([cur, opp])[np.newaxis, ...]\n        x_tensor = torch.tensor(x, dtype=torch.float32)\n        \n        # Inference\n        with torch.no_grad():\n            logits, _ = model(x_tensor)\n        probs = F.softmax(logits, dim=1).numpy().flatten()\n        \n        # Mask illegal moves\n        probs = [p if c in legal_moves else -10 for c, p in enumerate(probs)]\n        \n        return int(np.argmax(probs))\n    \n    except Exception:\n        # Fallback to center strategy\n        center_order = [3, 2, 4, 1, 5, 0, 6]\n        for col in center_order:\n            if col in legal_moves:\n                return col\n        return legal_moves[0] if legal_moves else 0\n\"\"\"\n\nwith open(\"submission.py\", \"w\") as f:\n    f.write(submission_code)\n    \nprint(\"Submission file created with trained AlphaZero agent!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Conclusion\n\nThis notebook tries to implement a baseline implementation of AlphaZero-style self-play reinforcement learning agent adapted to the ConnectX environment. Despite computational constraints and a reduced simulation/training scale compared to the original DeepMind setup, the implementation tries to preserves the essential components of AlphaZero and demonstrates measurable learning through self-play.\n\n*This notebook prioritizes architecture fidelity and learning over competition optimization.* It is not tuned for leaderboard performance but rather aims to faithfully reproduce core algorithmic ideas in a reproducible and requisite format.\n\nThis project represents an ongoing personal pursuit to explore and internalize the workings of modern reinforcement learning systems.\n\nThe current version reflects that process and is shared with the intention of contributing to the broader learning and research community. It serves as both a research exploration and an educational resource, and will continue to evolve as part of my long term learning journey.","metadata":{"_uuid":"f4f6d324-f061-46dd-a21e-de7c8b6a321e","_cell_guid":"a9c041d0-784a-44d5-b7bd-29f23ccff139","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}