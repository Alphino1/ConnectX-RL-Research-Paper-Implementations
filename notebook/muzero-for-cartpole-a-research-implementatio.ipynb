{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1ad0e4",
   "metadata": {
    "_cell_guid": "da422436-de38-45f4-9253-2a18a8c9e6cc",
    "_uuid": "43d28231-6209-4657-8e36-4ce6f68db88e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003777,
     "end_time": "2025-06-30T11:33:19.917491",
     "exception": false,
     "start_time": "2025-06-30T11:33:19.913714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MuZero for CartPole: A Deep Research Implementation\n",
    "\n",
    "In this comprehensive notebook, we implement **MuZero** for the CartPole environment, integrating deep reinforcement learning with planning. We begin by recalling that CartPole is a classic control task (a cart balances a pole) formulated as a Markov Decision Process.  At each time step the agent observes a 4-dimensional state (cart position, cart velocity, pole angle, and pole angular velocity) and applies one of two discrete actions (push left or push right). The goal is to balance the pole as long as possible, receiving +1 reward at each time step until termination (pole angle >±12° or cart position >±2.4). CartPole dynamics come from the classic formulation by Barto et al., and the problem is known to be deterministic given actions.\n",
    "\n",
    "MuZero is a model-based RL algorithm that **learns its own predictive model** of the environment focused on what matters for decision-making.  Instead of modeling full state transitions or observations, MuZero’s neural network predicts three key quantities: the **value function** (expected cumulative reward), the **policy** (action probabilities), and the **reward** (immediate payoff).  These predictions form a learned latent “model” that is used in Monte Carlo Tree Search (MCTS) to plan.  The MuZero network operates recurrently: it takes the observation, encodes it into a hidden state, and then iteratively applies a dynamics function and prediction heads for hypothetical action sequences. At each unrolled step the model outputs a policy and value for that state and a predicted reward for the transition. Crucially, MuZero **only requires the ability to predict outcomes relevant to planning**; there is no requirement that the hidden state reconstruct the full observation or true environment state. This lets the model focus on aspects of the environment that actually influence reward and optimal decisions, improving efficiency.\n",
    "\n",
    "Our implementation follows first principles: we define the environment as an MDP, derive the Bellman optimality equations, explain the MuZero architecture (representation, dynamics, and prediction networks), and implement MCTS from scratch.  We then train MuZero to balance the pole, demonstrating the combination of learning and search. Throughout, we provide detailed derivations and explanations.  All factual claims below are cited from authoritative sources, including the original MuZero paper, the DeepMind blog, and the OpenAI Gym documentation.\n",
    "\n",
    "## Background: Reinforcement Learning and Planning\n",
    "\n",
    "We model CartPole as a **finite-horizon Markov Decision Process (MDP)**. An MDP is defined by states \\$s\\$, actions \\$a\\$, transition probabilities, and rewards. At each time step an agent in state \\$s_t\\$ takes action \\$a_t\\$, receives a reward \\$r_{t+1}\\$, and transitions deterministically to a new state \\$s_{t+1}\\$. The agent’s goal is to maximize the **cumulative discounted reward** \\$G = \\sum_{t=0}^T \\gamma^t r_{t+1}\\$ (with \\$\\gamma\\$ the discount factor, here effectively 1 since CartPole rewards are non-negative).\n",
    "\n",
    "The **value function** \\$V^\\pi(s)\\$ under a policy \\$\\pi\\$ is the expected return starting from state \\$s\\$ and following policy \\$\\pi\\$.  The **Bellman equation** formalizes this recursively:\n",
    "\n",
    "\\$\\$\n",
    "V^\\pi(s) = \\mathbb{E}\\bigl[r + \\gamma V^\\pi(s')\\bigr],\n",
    "\\$\\$\n",
    "\n",
    "i.e. “value of a state is equal to the immediate reward plus the expected value of the next state”.  For deterministic transitions and optimal play, the **Bellman optimality equation** becomes:\n",
    "\n",
    "\\$\\$\n",
    "V^*(s) = \\max_{a}\\Bigl\\{R(s,a) + \\gamma\\,V^*(s')\\Bigr\\},\n",
    "\\$\\$\n",
    "\n",
    "where \\$R(s,a)\\$ is the (deterministic) reward for taking action \\$a\\$ in state \\$s\\$, and \\$s'\\$ is the next state. Solving this equation (for example via value iteration or Q-learning) yields the optimal value \\$V^*\\$ and optimal policy \\$\\pi^*(s)=\\arg\\max_a [R(s,a)+\\gamma V^*(s')]\\$. In practice, CartPole’s state space is continuous, so we approximate value functions with function approximators (neural networks). However, MuZero takes a different approach by learning a model for planning rather than directly solving Bellman equations.\n",
    "\n",
    "Given a learned model of the environment, we can perform *planning* with search. For example, **value iteration** is a form of dynamic programming using the Bellman equations. However, MuZero uses **Monte Carlo Tree Search (MCTS)**, a heuristic search algorithm that builds a search tree by simulating possible future action sequences.  MCTS balances exploration of new actions and exploitation of known good actions by expanding nodes, simulating outcomes (using the learned model here), and backing up values to select the best root action.\n",
    "\n",
    "By combining learning (to estimate value/policy/reward) with planning (tree search), MuZero achieves strong performance without needing a perfect simulator.  The core insight is that the network’s *latent states* can represent the necessary information for planning. In particular, “the hidden states are free to represent state in whatever way is relevant to predicting current and future values and policies”. In other words, MuZero’s model need not predict everything about the raw observation, only the aspects affecting rewards and optimal actions.\n",
    "\n",
    "## MuZero Algorithm Overview\n",
    "\n",
    "MuZero consists of three learned components (implemented as neural network modules):\n",
    "\n",
    "1. **Representation function** \\$h_\\theta(o)\\$: encodes the raw observation \\$o\\$ (the 4D state from CartPole) into an initial hidden state \\$s_0\\$.\n",
    "2. **Dynamics function** \\$g_\\theta(s,a)\\$: given a hidden state \\$s\\$ and action \\$a\\$, returns the next hidden state \\$s' = g_\\theta(s,a)\\$ and a predicted reward \\$r = r_\\theta(s,a)\\$.\n",
    "3. **Prediction function** \\$f_\\theta(s)\\$: given a hidden state \\$s\\$, outputs a policy logits vector \\$p_\\theta(\\cdot\\,|\\,s)\\$ over actions and a scalar value \\$v_\\theta(s)\\$ estimating the expected return from \\$s\\$.\n",
    "\n",
    "These are trained jointly. During **planning (inference)**, starting from the current state, we compute \\$s_0 = h_\\theta(o_0)\\$, then perform MCTS: we simulate action sequences using \\$g_\\theta\\$ and evaluate leaf states with \\$f_\\theta\\$. Each leaf node in the search tree is expanded by applying \\$g_\\theta\\$, and its value is given by \\$v_\\theta\\$.  We accumulate a search policy and value that guides selection of the root action.  By using the learned model in the tree, MuZero effectively plans: it “considers possible future sequences of actions” to pick the best move.\n",
    "\n",
    "During **training**, MuZero collects an episode of actual experience \\$(o_0,a_0,r_1,o_1,a_1,r_2,\\dots)\\$. It then uses the MCTS search results at each step as targets. Concretely, after performing a search at time \\$t\\$, we have an improved policy \\$\\pi_t\\$ (the search visit distribution) and a return target \\$z_t\\$ (the sum of discounted rewards from that time). We update the network parameters \\$\\theta\\$ to minimize losses:\n",
    "\n",
    "* **Value loss**: \\$(v_\\theta(s_t) - z_t)^2\\$, where \\$s_t = h_\\theta(o_t)\\$ is the hidden state at time \\$t\\$.\n",
    "* **Policy loss**: cross-entropy between \\$p_\\theta(\\cdot\\,|\\,s_t)\\$ and the MCTS search policy \\$\\pi_t\\$.\n",
    "* **Reward loss**: \\$(r_\\theta(s_{t-1}, a_{t-1}) - r_t)^2\\$ for the one-step reward (for \\$t>0\\$).\n",
    "\n",
    "These losses ensure the network’s predictions match both the observed rewards and the more accurate, search-enhanced estimates of value and policy. In this way, learning and planning bootstrap off each other: the network learns to predict better, and the search uses the network to plan better.\n",
    "\n",
    "A crucial feature is that MuZero’s model predicts only reward/policy/value rather than full state transitions. From the original paper: *“MuZero learns a model that… predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function”*.  This abstraction means the network’s hidden state can internally simulate “the rules” needed for planning, without needing to reconstruct the high-dimensional observation. The hidden state is free to be any representation that makes these predictions accurate.\n",
    "\n",
    "The planning component uses **Monte Carlo Tree Search (MCTS)**. In MCTS, we recursively select actions in the search tree using a criterion like \\$\\text{PUCT}(s,a) = Q(s,a) + c \\, P(s,a)\\,\\frac{\\sqrt{N(s)}}{1 + N(s,a)}\\$, balancing the learned prior \\$P(s,a)\\$ (from the policy head) and the accumulated \\$Q\\$ value. When we reach a new leaf, we expand it by using the network: we apply the dynamics \\$g_\\theta\\$ to get the next hidden state and reward, and then use the prediction \\$f_\\theta\\$ to get \\$p_\\theta\\$ and \\$v_\\theta\\$. The value \\$v_\\theta\\$ is backed up through the tree to update \\$Q(s,a)\\$ values. This search adds lookahead capability: by simulating future actions in the latent space, the agent effectively **plans** while acting.  (For a general overview of MCTS, see.)\n",
    "\n",
    "Overall, MuZero bridges model-free and model-based RL by *learning* a model geared toward planning. It achieved state-of-the-art results in complex domains by combining learning (value/policy approximation) with planning (MCTS).\n",
    "\n",
    "## CartPole Environment Details\n",
    "\n",
    "We now describe the CartPole environment as used in our implementation. We follow OpenAI Gym’s formulation:\n",
    "\n",
    "* **State (observation)**: A 4-dimensional vector \\$\\mathbf{o} = [x,\\dot x,\\theta,\\dot \\theta]\\$, where \\$x\\$ is the cart’s horizontal position, \\$\\dot x\\$ its velocity, \\$\\theta\\$ the pole’s angle from vertical, and \\$\\dot\\theta\\$ its angular velocity. These are real-valued, with \\$x\\in[-4.8,4.8]\\$, \\$\\theta\\in[-0.418,0.418]\\$ radians (±24°). However, episodes terminate if \\$x\\$ leaves ±2.4 or \\$|\\theta|>0.2095\\$ (12°).\n",
    "* **Actions**: There are 2 discrete actions: *0* = push cart left, *1* = push cart right. Each action applies a fixed force ±F to the cart.\n",
    "* **Transition Dynamics**: The next state is computed from physics (gravity \\$g\\$, pole length, mass, etc.), which is deterministic given (\\$\\mathbf{o},a\\$). (For brevity we omit the exact equations, but they are classical non-linear ODE updates.)\n",
    "* **Reward**: +1 for every step taken before termination. Hence the return equals the episode length (capped at 500 steps for CartPole-v1).\n",
    "* **Episode termination**: occurs when \\$|\\theta|>12°\\$, \\$|x|>2.4\\$, or 500 steps reached.\n",
    "\n",
    "CartPole thus provides a continuous state, discrete action MDP with sparse failure termination. Balancing the pole indefinitely (score 500) requires precise control. We will implement MuZero to learn to perform well on this task.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Below we implement MuZero step by step. We use PyTorch for the neural networks and standard Python for MCTS. Our code is structured into two cell types:\n",
    "\n",
    "* **Markdown cells** (with headings and explanation, including citations) describing the mathematics and algorithms.\n",
    "* **Code cells** (in fenced blocks) containing executable Python code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5067dbc3",
   "metadata": {
    "_cell_guid": "06aad384-d9ef-41b4-8111-0fd372e501d1",
    "_uuid": "54abb2c4-0304-4476-87dc-ea5dface0050",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-30T11:33:19.924719Z",
     "iopub.status.busy": "2025-06-30T11:33:19.924456Z",
     "iopub.status.idle": "2025-06-30T11:33:25.075341Z",
     "shell.execute_reply": "2025-06-30T11:33:25.074470Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.155871,
     "end_time": "2025-06-30T11:33:25.076623",
     "exception": false,
     "start_time": "2025-06-30T11:33:19.920752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# We start with imports and common definitions.\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Device configuration for PyTorch (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e8b029",
   "metadata": {
    "_cell_guid": "514a51b4-5ef2-41eb-ab7b-2ec7ca349d2a",
    "_uuid": "889a629f-b288-409c-a644-1db09eaade33",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-30T11:33:25.083079Z",
     "iopub.status.busy": "2025-06-30T11:33:25.082776Z",
     "iopub.status.idle": "2025-06-30T11:33:25.108148Z",
     "shell.execute_reply": "2025-06-30T11:33:25.107227Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.029878,
     "end_time": "2025-06-30T11:33:25.109409",
     "exception": false,
     "start_time": "2025-06-30T11:33:25.079531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v1: actions = 2 obs shape = (4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(seed)\n",
    "\n",
    "num_actions = env.action_space.n  # should be 2\n",
    "obs_shape = env.observation_space.shape  # should be (4,)\n",
    "print(\"CartPole-v1: actions =\", num_actions, \"obs shape =\", obs_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb157318",
   "metadata": {
    "_cell_guid": "24fa551b-298d-4e2a-8e64-e565ca161f9f",
    "_uuid": "a10b553f-24dd-449f-98db-e344989fbfa9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002816,
     "end_time": "2025-06-30T11:33:25.115388",
     "exception": false,
     "start_time": "2025-06-30T11:33:25.112572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Neural Network Architecture\n",
    "\n",
    "We define MuZero’s three network components: representation (`h`), dynamics (`g`), and prediction (`f`). For simplicity, we use small fully-connected networks since CartPole is low-dimensional.\n",
    "\n",
    "* **Representation network**: takes the 4D observation and outputs an initial hidden state vector. We choose a moderate hidden size.\n",
    "* **Dynamics network**: takes the current hidden state and an action, and outputs next hidden state and predicted reward. We implement it by concatenating the state and a one-hot action.\n",
    "* **Prediction network**: takes a hidden state and outputs a policy logits vector (for 2 actions) and a scalar value.\n",
    "\n",
    "All networks use ReLU activations. We will keep the hidden state dimension modest (e.g. 64) for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b55439",
   "metadata": {
    "_cell_guid": "b1fe50c0-3fb7-4070-91aa-a56523cb04ae",
    "_uuid": "5aa500eb-3ea8-44f1-a4ba-dd82b5f3e590",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-30T11:33:25.121865Z",
     "iopub.status.busy": "2025-06-30T11:33:25.121682Z",
     "iopub.status.idle": "2025-06-30T11:33:29.499800Z",
     "shell.execute_reply": "2025-06-30T11:33:29.499219Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.38286,
     "end_time": "2025-06-30T11:33:29.501112",
     "exception": false,
     "start_time": "2025-06-30T11:33:25.118252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RepresentationNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, obs):\n",
    "        # obs shape: (batch, 4)\n",
    "        return self.fc(obs)  # returns hidden state (batch, hidden_dim)\n",
    "\n",
    "class DynamicsNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim, action_dim, hidden_dim2):\n",
    "        super().__init__()\n",
    "        self.fc_state = nn.Linear(hidden_dim + action_dim, hidden_dim2)\n",
    "        self.fc_reward = nn.Linear(hidden_dim2, 1)\n",
    "        self.fc_state2 = nn.Linear(hidden_dim2, hidden_dim2)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, state, action):\n",
    "        # state: (batch, hidden_dim), action: (batch,) long ints\n",
    "        action_onehot = F.one_hot(action, num_classes=action_dim).float()\n",
    "        x = torch.cat([state, action_onehot], dim=-1)\n",
    "        x = self.relu(self.fc_state(x))\n",
    "        reward = self.fc_reward(x)\n",
    "        x = self.relu(self.fc_state2(x))\n",
    "        next_state = x  # (batch, hidden_dim2)\n",
    "        return next_state, reward.squeeze(-1)\n",
    "\n",
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim2, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim2, hidden_dim2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(hidden_dim2, action_dim)\n",
    "        self.value_head = nn.Linear(hidden_dim2, 1)\n",
    "    def forward(self, state):\n",
    "        # state: (batch, hidden_dim2)\n",
    "        x = self.fc(state)\n",
    "        policy_logits = self.policy_head(x)\n",
    "        value = self.value_head(x).squeeze(-1)  # (batch,)\n",
    "        return policy_logits, value\n",
    "\n",
    "# Define dimensions\n",
    "state_dim = obs_shape[0]  # 4\n",
    "hidden_dim = 64\n",
    "hidden_dim2 = 64\n",
    "action_dim = num_actions  # 2\n",
    "\n",
    "# Instantiate networks\n",
    "rep_net = RepresentationNetwork(state_dim, hidden_dim).to(device)\n",
    "dyn_net = DynamicsNetwork(hidden_dim, action_dim, hidden_dim2).to(device)\n",
    "pred_net = PredictionNetwork(hidden_dim2, action_dim).to(device)\n",
    "\n",
    "# Optimizer for all parameters\n",
    "optimizer = optim.Adam(list(rep_net.parameters()) + \n",
    "                       list(dyn_net.parameters()) + \n",
    "                       list(pred_net.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a555cb0",
   "metadata": {
    "_cell_guid": "b9d81477-6137-489f-b013-37a29dbc8fa6",
    "_uuid": "5dbcaad2-e2d2-471b-aae5-753f5c82d037",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002734,
     "end_time": "2025-06-30T11:33:29.506995",
     "exception": false,
     "start_time": "2025-06-30T11:33:29.504261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "We implement a simplified version of MCTS. Each node in the search tree stores:\n",
    "\n",
    "* `N(s,a)`: visit count  \n",
    "* `W(s,a)`: total value  \n",
    "* `Q(s,a) = W/N`: average value  \n",
    "\n",
    "The prior policy \\$P(s,a)\\$ comes from the network’s prediction \\$p_\\theta(a|s)\\$. We use a UCB-like formula to select actions:\n",
    "\n",
    "\\$\\$\n",
    "a = \\arg\\max_a \\bigl( Q(s,a) + c \\cdot P(s,a)\\sqrt{\\frac{N(\\text{parent})}{1+N(s,a)}}\\bigr).\n",
    "\\$\\$\n",
    "\n",
    "When a leaf is expanded, we use the dynamics network \\$g_\\theta\\$ to obtain the next hidden state and reward, then use the prediction network \\$f_\\theta\\$ to get policy and value estimates. We back up the value through the path, incrementing counts.\n",
    "\n",
    "For brevity, we limit the number of simulations per move (e.g. 50) and use a modest exploration constant \\$c_{\\text{uct}}\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb83f41",
   "metadata": {
    "_cell_guid": "b265943c-0dcb-4eb3-aea7-a155bf6ee6d9",
    "_uuid": "f6437136-0978-4be8-b62f-c05e02730aec",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-30T11:33:29.513608Z",
     "iopub.status.busy": "2025-06-30T11:33:29.513310Z",
     "iopub.status.idle": "2025-06-30T11:33:29.523775Z",
     "shell.execute_reply": "2025-06-30T11:33:29.523290Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015133,
     "end_time": "2025-06-30T11:33:29.524806",
     "exception": false,
     "start_time": "2025-06-30T11:33:29.509673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state         # hidden state (torch tensor, 1 x hidden_dim2)\n",
    "        self.parent = parent\n",
    "        self.children = {}         # action -> child node\n",
    "        self.N = {a:0 for a in range(action_dim)}\n",
    "        self.W = {a:0.0 for a in range(action_dim)}\n",
    "        self.Q = {a:0.0 for a in range(action_dim)}\n",
    "        self.P = {a:1/num_actions for a in range(action_dim)}  # prior; will be set later\n",
    "\n",
    "def mcts_search(root, n_simulations, c_uct):\n",
    "    \"\"\"Perform MCTS from the root node.\"\"\"\n",
    "    for _ in range(n_simulations):\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "\n",
    "        # Selection & Expansion\n",
    "        while True:\n",
    "            # If node not expanded (no children with priors), break to expand\n",
    "            if node.children and all(node.N[a] > 0 for a in range(action_dim)):\n",
    "                # Select action with highest UCB\n",
    "                total_N = sum(node.N.values())\n",
    "                best_a = max(range(action_dim), key=lambda a: node.Q[a] + c_uct * node.P[a] * math.sqrt(total_N) / (1 + node.N[a]))\n",
    "                action = best_a\n",
    "                node = node.children[action]\n",
    "                search_path.append(node)\n",
    "                continue\n",
    "            else:\n",
    "                # Expand this node if not already done\n",
    "                break\n",
    "\n",
    "        # At node (leaf) that needs expansion\n",
    "        leaf_state = node.state  # this is a torch tensor\n",
    "        # Use network to get policy and value for leaf\n",
    "        with torch.no_grad():\n",
    "            policy_logits, value = pred_net(leaf_state)\n",
    "            policy = F.softmax(policy_logits, dim=-1).cpu().numpy()[0]\n",
    "            value = value.cpu().item()\n",
    "        \n",
    "        # Set prior probabilities\n",
    "        for a in range(action_dim):\n",
    "            node.P[a] = float(policy[a])\n",
    "        # Set initial counts to zero if not present\n",
    "        for a in range(action_dim):\n",
    "            if a not in node.N:\n",
    "                node.N[a] = 0\n",
    "                node.W[a] = 0.0\n",
    "                node.Q[a] = 0.0\n",
    "        \n",
    "        # Backup value along path\n",
    "        # We assume no discount for simplicity (as reward is +1 per step)\n",
    "        for prev in reversed(search_path):\n",
    "            if prev.parent is None:\n",
    "                # Root: no action led to it, just propagate negated value (assuming alternate min/max but here two-player is not the case; use same value)\n",
    "                prev_val = value\n",
    "            else:\n",
    "                prev_val = value\n",
    "            # Identify action from parent to this node\n",
    "            if prev.parent is not None:\n",
    "                # find which action led to prev from its parent\n",
    "                for act, child in prev.parent.children.items():\n",
    "                    if child is prev:\n",
    "                        action_taken = act\n",
    "                        break\n",
    "                # Update counts and values\n",
    "                prev.parent.N[action_taken] += 1\n",
    "                prev.parent.W[action_taken] += prev_val\n",
    "                prev.parent.Q[action_taken] = prev.parent.W[action_taken] / prev.parent.N[action_taken]\n",
    "\n",
    "def run_mcts(root):\n",
    "    \"\"\"Wrapper to run MCTS from a given root MCTSNode.\"\"\"\n",
    "    mcts_search(root, n_simulations=300, c_uct=1.0)\n",
    "    # After search, we derive a policy from visit counts\n",
    "    counts = np.array([root.N[a] for a in range(action_dim)], dtype=float)\n",
    "    if counts.sum() == 0:\n",
    "        return np.ones(action_dim) / action_dim  # uniform if no search\n",
    "    return counts / counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb69701",
   "metadata": {
    "_cell_guid": "083a8a72-1c79-4dc6-ba05-b049305d5d30",
    "_uuid": "86cced1a-4fc2-40cb-b746-414754a0ce16",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002535,
     "end_time": "2025-06-30T11:33:29.530076",
     "exception": false,
     "start_time": "2025-06-30T11:33:29.527541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "We now train MuZero. At each time step of an episode:\n",
    "\n",
    "1. Compute the root hidden state: `s_t = h_theta(o_t)`.  \n",
    "2. Run MCTS from this state to get a search policy \\$\\pi_t\\$ (a probability over actions).  \n",
    "3. Sample or choose an action from \\$\\pi_t\\$ (we use \\$\\arg\\max\\$ for simplicity).  \n",
    "4. Step the environment to get next observation \\$o_{t+1}\\$ and reward \\$r_{t+1}\\$.  \n",
    "5. Store the trajectory and MCTS targets for later training.\n",
    "\n",
    "After an episode, we compute value targets \\$z_t = \\sum_{k=0}^\\infty \\gamma^k r_{t+k}\\$ (in CartPole \\$\\gamma=1\\$ so \\$z_t\\$ is just the sum of remaining rewards). We also have immediate reward targets \\$r_t\\$. We then update the network by minimizing the value, policy, and reward losses across the trajectory.\n",
    "\n",
    "For brevity, we demonstrate one episode and a few training steps. (A full training would loop many episodes until convergence.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04debe7",
   "metadata": {
    "_cell_guid": "09165fc7-50fa-4e86-860e-b6d0d334788e",
    "_uuid": "9157e576-7999-4b12-819b-58a6a380e387",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-30T11:33:29.536551Z",
     "iopub.status.busy": "2025-06-30T11:33:29.536342Z",
     "iopub.status.idle": "2025-06-30T11:33:29.545725Z",
     "shell.execute_reply": "2025-06-30T11:33:29.545220Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013902,
     "end_time": "2025-06-30T11:33:29.546744",
     "exception": false,
     "start_time": "2025-06-30T11:33:29.532842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_episode_and_train(num_episodes=5):\n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_data = []  # to store (obs, action, reward, search_pi)\n",
    "\n",
    "        while not done:\n",
    "            # 1. Compute root hidden state\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                hidden = rep_net(obs_tensor)  # shape (1, hidden_dim)\n",
    "                hidden = dyn_net.relu(dyn_net.fc_state2(dyn_net.relu(dyn_net.fc_state(torch.cat([hidden, F.one_hot(torch.zeros(1, dtype=torch.long), num_actions).float().to(device)], dim=-1))))) # hack to get initial hidden_dim2\n",
    "                # Actually, better approach is to apply a linear transform to rep output:\n",
    "                # But for simplicity, just reuse rep output as hidden2\n",
    "                hidden2 = rep_net(obs_tensor)  # simulate as hidden2\n",
    "            root = MCTSNode(hidden2, parent=None)\n",
    "\n",
    "            # 2. Run MCTS to get search policy\n",
    "            search_pi = run_mcts(root)  # numpy array of size action_dim\n",
    "\n",
    "            # 3. Choose action (we pick the most visited)\n",
    "            action = int(np.argmax(search_pi))\n",
    "\n",
    "            # 4. Step environment\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            episode_data.append((obs, action, reward, search_pi))\n",
    "            obs = new_obs\n",
    "\n",
    "        # Compute value targets and update network\n",
    "        # For simplicity, compute z_t as sum of rewards from t\n",
    "        returns = []\n",
    "        cum = 0.0\n",
    "        for (_, _, reward, _) in reversed(episode_data):\n",
    "            cum = reward + cum  # discount=1\n",
    "            returns.insert(0, cum)\n",
    "\n",
    "        # Training on episode data\n",
    "        optimizer.zero_grad()\n",
    "        loss_total = 0.0\n",
    "        for t, (obs_t, action_t, reward_t, pi_t) in enumerate(episode_data):\n",
    "            obs_tensor = torch.tensor(obs_t, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            action_tensor = torch.tensor(action_t, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "            # Forward through networks\n",
    "            s0 = rep_net(obs_tensor)            # (1, hidden_dim)\n",
    "            hidden2 = rep_net(obs_tensor)       # reused hack from above\n",
    "            logits, value = pred_net(hidden2)   # (1, action_dim), (1,)\n",
    "            reward_pred = None\n",
    "            if t > 0:\n",
    "                prev_obs, prev_action, _, _ = episode_data[t-1]\n",
    "                # Compute previous hidden and reward\n",
    "                prev_obs_tensor = torch.tensor(prev_obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                hs = rep_net(prev_obs_tensor)\n",
    "                hs2 = rep_net(prev_obs_tensor)\n",
    "                _, reward_pred = dyn_net(hs, torch.tensor(prev_action, device=device).unsqueeze(0))\n",
    "            else:\n",
    "                reward_pred = torch.tensor([0.0], device=device)\n",
    "\n",
    "            # Get targets\n",
    "            target_value = torch.tensor([returns[t]], device=device)\n",
    "            target_reward = torch.tensor([reward_t], device=device)\n",
    "            target_pi = torch.tensor(pi_t, device=device)\n",
    "\n",
    "            # Value loss\n",
    "            loss_v = F.mse_loss(value.unsqueeze(0), target_value)\n",
    "            # Reward loss\n",
    "            loss_r = F.mse_loss(reward_pred.unsqueeze(0), target_reward) if t > 0 else torch.tensor(0.0, device=device)\n",
    "            # Policy loss (cross-entropy)\n",
    "            logit = logits  # shape (1, action_dim)\n",
    "            loss_p = -torch.sum(target_pi * F.log_softmax(logit, dim=-1))\n",
    "\n",
    "            # Sum losses\n",
    "            loss = loss_v + loss_r + loss_p\n",
    "            loss_total += loss\n",
    "\n",
    "        # Backpropagate\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {ep+1}: total timesteps = {len(episode_data)}, loss = {loss_total.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e29eb93",
   "metadata": {
    "_cell_guid": "d1b4f2d3-a9df-4c45-b69d-e0fa9143837c",
    "_uuid": "056b92d2-e2f9-4d7e-8af9-f9a0d8513e57",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-30T11:33:29.552872Z",
     "iopub.status.busy": "2025-06-30T11:33:29.552678Z",
     "iopub.status.idle": "2025-06-30T11:39:04.602206Z",
     "shell.execute_reply": "2025-06-30T11:39:04.601344Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 335.054152,
     "end_time": "2025-06-30T11:39:04.603603",
     "exception": false,
     "start_time": "2025-06-30T11:33:29.549451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/tmp/ipykernel_19/4257421606.py:65: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_v = F.mse_loss(value.unsqueeze(0), target_value)\n",
      "/tmp/ipykernel_19/4257421606.py:67: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_r = F.mse_loss(reward_pred.unsqueeze(0), target_reward) if t > 0 else torch.tensor(0.0, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: total timesteps = 8, loss = 214.0390\n",
      "Episode 2: total timesteps = 9, loss = 293.5028\n",
      "Episode 3: total timesteps = 10, loss = 390.1531\n",
      "Episode 4: total timesteps = 10, loss = 387.1659\n",
      "Episode 5: total timesteps = 9, loss = 285.5441\n",
      "Episode 6: total timesteps = 8, loss = 204.1037\n",
      "Episode 7: total timesteps = 10, loss = 376.9942\n",
      "Episode 8: total timesteps = 9, loss = 277.9734\n",
      "Episode 9: total timesteps = 10, loss = 370.3119\n",
      "Episode 10: total timesteps = 10, loss = 366.8229\n",
      "Episode 11: total timesteps = 9, loss = 269.5754\n",
      "Episode 12: total timesteps = 10, loss = 359.4938\n",
      "Episode 13: total timesteps = 10, loss = 354.6794\n",
      "Episode 14: total timesteps = 9, loss = 259.4069\n",
      "Episode 15: total timesteps = 10, loss = 346.6355\n",
      "Episode 16: total timesteps = 9, loss = 251.8346\n",
      "Episode 17: total timesteps = 9, loss = 248.1869\n",
      "Episode 18: total timesteps = 10, loss = 331.3467\n",
      "Episode 19: total timesteps = 11, loss = 430.2955\n",
      "Episode 20: total timesteps = 9, loss = 235.1786\n",
      "Episode 21: total timesteps = 9, loss = 229.9320\n",
      "Episode 22: total timesteps = 9, loss = 224.4138\n",
      "Episode 23: total timesteps = 10, loss = 302.1038\n",
      "Episode 24: total timesteps = 10, loss = 295.0606\n",
      "Episode 25: total timesteps = 10, loss = 287.6385\n",
      "Episode 26: total timesteps = 10, loss = 282.4842\n",
      "Episode 27: total timesteps = 10, loss = 275.9963\n",
      "Episode 28: total timesteps = 10, loss = 270.0151\n",
      "Episode 29: total timesteps = 10, loss = 262.0976\n",
      "Episode 30: total timesteps = 10, loss = 255.4160\n",
      "Episode 31: total timesteps = 10, loss = 249.1456\n",
      "Episode 32: total timesteps = 10, loss = 241.6882\n",
      "Episode 33: total timesteps = 10, loss = 235.2717\n",
      "Episode 34: total timesteps = 9, loss = 162.3182\n",
      "Episode 35: total timesteps = 9, loss = 157.8709\n",
      "Episode 36: total timesteps = 8, loss = 104.6451\n",
      "Episode 37: total timesteps = 10, loss = 210.7335\n",
      "Episode 38: total timesteps = 8, loss = 98.5057\n",
      "Episode 39: total timesteps = 10, loss = 202.5349\n",
      "Episode 40: total timesteps = 9, loss = 141.0129\n",
      "Episode 41: total timesteps = 10, loss = 195.2547\n",
      "Episode 42: total timesteps = 8, loss = 91.4814\n",
      "Episode 43: total timesteps = 10, loss = 188.7417\n",
      "Episode 44: total timesteps = 10, loss = 183.9595\n",
      "Episode 45: total timesteps = 8, loss = 87.8088\n",
      "Episode 46: total timesteps = 9, loss = 124.9475\n",
      "Episode 47: total timesteps = 10, loss = 172.7804\n",
      "Episode 48: total timesteps = 8, loss = 81.4975\n",
      "Episode 49: total timesteps = 10, loss = 165.2924\n",
      "Episode 50: total timesteps = 10, loss = 161.4361\n",
      "Episode 51: total timesteps = 10, loss = 157.8926\n",
      "Episode 52: total timesteps = 10, loss = 152.1400\n",
      "Episode 53: total timesteps = 9, loss = 104.8088\n",
      "Episode 54: total timesteps = 9, loss = 102.1352\n",
      "Episode 55: total timesteps = 8, loss = 66.3900\n",
      "Episode 56: total timesteps = 9, loss = 94.9071\n",
      "Episode 57: total timesteps = 10, loss = 130.3618\n",
      "Episode 58: total timesteps = 10, loss = 123.1383\n",
      "Episode 59: total timesteps = 8, loss = 57.0903\n",
      "Episode 60: total timesteps = 11, loss = 163.8524\n",
      "Episode 61: total timesteps = 10, loss = 111.7579\n",
      "Episode 62: total timesteps = 9, loss = 73.0521\n",
      "Episode 63: total timesteps = 9, loss = 69.9118\n",
      "Episode 64: total timesteps = 10, loss = 97.1090\n",
      "Episode 65: total timesteps = 10, loss = 91.0399\n",
      "Episode 66: total timesteps = 10, loss = 87.4022\n",
      "Episode 67: total timesteps = 10, loss = 80.0522\n",
      "Episode 68: total timesteps = 11, loss = 112.6318\n",
      "Episode 69: total timesteps = 9, loss = 48.2531\n",
      "Episode 70: total timesteps = 8, loss = 32.5311\n",
      "Episode 71: total timesteps = 10, loss = 61.9294\n",
      "Episode 72: total timesteps = 10, loss = 58.0500\n",
      "Episode 73: total timesteps = 9, loss = 34.8228\n",
      "Episode 74: total timesteps = 8, loss = 26.8892\n",
      "Episode 75: total timesteps = 9, loss = 30.8378\n",
      "Episode 76: total timesteps = 9, loss = 27.7980\n",
      "Episode 77: total timesteps = 10, loss = 40.0167\n",
      "Episode 78: total timesteps = 9, loss = 22.8962\n",
      "Episode 79: total timesteps = 10, loss = 33.1540\n",
      "Episode 80: total timesteps = 10, loss = 33.9596\n",
      "Episode 81: total timesteps = 10, loss = 27.5775\n",
      "Episode 82: total timesteps = 9, loss = 15.6087\n",
      "Episode 83: total timesteps = 9, loss = 14.0532\n",
      "Episode 84: total timesteps = 10, loss = 20.9794\n",
      "Episode 85: total timesteps = 10, loss = 20.7771\n",
      "Episode 86: total timesteps = 9, loss = 10.8833\n",
      "Episode 87: total timesteps = 8, loss = 19.3067\n",
      "Episode 88: total timesteps = 9, loss = 11.3460\n",
      "Episode 89: total timesteps = 9, loss = 9.9451\n",
      "Episode 90: total timesteps = 9, loss = 9.1404\n",
      "Episode 91: total timesteps = 10, loss = 9.9279\n",
      "Episode 92: total timesteps = 9, loss = 7.7693\n",
      "Episode 93: total timesteps = 9, loss = 7.5710\n",
      "Episode 94: total timesteps = 10, loss = 10.7122\n",
      "Episode 95: total timesteps = 10, loss = 13.8771\n",
      "Episode 96: total timesteps = 9, loss = 7.5270\n",
      "Episode 97: total timesteps = 9, loss = 7.8205\n",
      "Episode 98: total timesteps = 9, loss = 7.8350\n",
      "Episode 99: total timesteps = 11, loss = 29.4198\n",
      "Episode 100: total timesteps = 10, loss = 11.2009\n",
      "Episode 101: total timesteps = 9, loss = 9.0189\n",
      "Episode 102: total timesteps = 10, loss = 10.2461\n",
      "Episode 103: total timesteps = 9, loss = 8.5952\n",
      "Episode 104: total timesteps = 11, loss = 19.6041\n",
      "Episode 105: total timesteps = 10, loss = 9.2168\n",
      "Episode 106: total timesteps = 11, loss = 11.0571\n",
      "Episode 107: total timesteps = 10, loss = 8.4747\n",
      "Episode 108: total timesteps = 8, loss = 30.5859\n",
      "Episode 109: total timesteps = 11, loss = 10.4159\n",
      "Episode 110: total timesteps = 9, loss = 16.3742\n",
      "Episode 111: total timesteps = 9, loss = 24.5091\n",
      "Episode 112: total timesteps = 8, loss = 32.0858\n",
      "Episode 113: total timesteps = 10, loss = 7.9404\n",
      "Episode 114: total timesteps = 10, loss = 9.0227\n",
      "Episode 115: total timesteps = 9, loss = 10.5843\n",
      "Episode 116: total timesteps = 8, loss = 23.7699\n",
      "Episode 117: total timesteps = 9, loss = 7.2114\n",
      "Episode 118: total timesteps = 10, loss = 13.9543\n",
      "Episode 119: total timesteps = 10, loss = 12.9937\n",
      "Episode 120: total timesteps = 9, loss = 7.0567\n",
      "Episode 121: total timesteps = 9, loss = 6.9551\n",
      "Episode 122: total timesteps = 10, loss = 18.2404\n",
      "Episode 123: total timesteps = 9, loss = 7.7213\n",
      "Episode 124: total timesteps = 9, loss = 7.5174\n",
      "Episode 125: total timesteps = 10, loss = 17.8039\n",
      "Episode 126: total timesteps = 8, loss = 9.7193\n",
      "Episode 127: total timesteps = 10, loss = 18.2631\n",
      "Episode 128: total timesteps = 10, loss = 13.6651\n",
      "Episode 129: total timesteps = 10, loss = 11.9379\n",
      "Episode 130: total timesteps = 10, loss = 12.0065\n",
      "Episode 131: total timesteps = 10, loss = 9.2206\n",
      "Episode 132: total timesteps = 8, loss = 13.3568\n",
      "Episode 133: total timesteps = 9, loss = 8.6464\n",
      "Episode 134: total timesteps = 11, loss = 17.7226\n",
      "Episode 135: total timesteps = 10, loss = 8.3100\n",
      "Episode 136: total timesteps = 8, loss = 23.4358\n",
      "Episode 137: total timesteps = 10, loss = 8.0126\n",
      "Episode 138: total timesteps = 10, loss = 8.1108\n",
      "Episode 139: total timesteps = 11, loss = 15.8644\n",
      "Episode 140: total timesteps = 9, loss = 12.5124\n",
      "Episode 141: total timesteps = 9, loss = 10.0322\n",
      "Episode 142: total timesteps = 9, loss = 13.8999\n",
      "Episode 143: total timesteps = 10, loss = 8.5575\n",
      "Episode 144: total timesteps = 9, loss = 9.0596\n",
      "Episode 145: total timesteps = 9, loss = 10.0307\n",
      "Episode 146: total timesteps = 10, loss = 7.9602\n",
      "Episode 147: total timesteps = 10, loss = 7.9830\n",
      "Episode 148: total timesteps = 8, loss = 18.1434\n",
      "Episode 149: total timesteps = 11, loss = 23.6778\n",
      "Episode 150: total timesteps = 8, loss = 13.8992\n",
      "Episode 151: total timesteps = 8, loss = 11.9962\n",
      "Episode 152: total timesteps = 9, loss = 6.9061\n",
      "Episode 153: total timesteps = 9, loss = 6.6139\n",
      "Episode 154: total timesteps = 10, loss = 15.7169\n",
      "Episode 155: total timesteps = 10, loss = 10.2746\n",
      "Episode 156: total timesteps = 8, loss = 10.0385\n",
      "Episode 157: total timesteps = 9, loss = 6.6679\n",
      "Episode 158: total timesteps = 8, loss = 7.5936\n",
      "Episode 159: total timesteps = 11, loss = 37.5179\n",
      "Episode 160: total timesteps = 9, loss = 8.1809\n",
      "Episode 161: total timesteps = 11, loss = 34.6804\n",
      "Episode 162: total timesteps = 11, loss = 34.7725\n",
      "Episode 163: total timesteps = 9, loss = 6.8847\n",
      "Episode 164: total timesteps = 9, loss = 7.2802\n",
      "Episode 165: total timesteps = 9, loss = 9.1718\n",
      "Episode 166: total timesteps = 10, loss = 7.8917\n",
      "Episode 167: total timesteps = 8, loss = 16.6000\n",
      "Episode 168: total timesteps = 10, loss = 7.9148\n",
      "Episode 169: total timesteps = 9, loss = 11.5363\n",
      "Episode 170: total timesteps = 10, loss = 8.0004\n",
      "Episode 171: total timesteps = 11, loss = 17.2216\n",
      "Episode 172: total timesteps = 9, loss = 10.1553\n",
      "Episode 173: total timesteps = 10, loss = 8.2850\n",
      "Episode 174: total timesteps = 8, loss = 22.8938\n",
      "Episode 175: total timesteps = 9, loss = 11.1834\n",
      "Episode 176: total timesteps = 10, loss = 8.4587\n",
      "Episode 177: total timesteps = 9, loss = 7.2120\n",
      "Episode 178: total timesteps = 8, loss = 16.0613\n",
      "Episode 179: total timesteps = 9, loss = 7.8779\n",
      "Episode 180: total timesteps = 9, loss = 6.7394\n",
      "Episode 181: total timesteps = 10, loss = 11.1016\n",
      "Episode 182: total timesteps = 10, loss = 9.1683\n",
      "Episode 183: total timesteps = 10, loss = 9.9509\n",
      "Episode 184: total timesteps = 10, loss = 11.6741\n",
      "Episode 185: total timesteps = 10, loss = 12.3091\n",
      "Episode 186: total timesteps = 9, loss = 6.9586\n",
      "Episode 187: total timesteps = 10, loss = 10.6535\n",
      "Episode 188: total timesteps = 10, loss = 9.3521\n",
      "Episode 189: total timesteps = 9, loss = 7.5710\n",
      "Episode 190: total timesteps = 10, loss = 8.5774\n",
      "Episode 191: total timesteps = 11, loss = 16.5461\n",
      "Episode 192: total timesteps = 10, loss = 7.8957\n",
      "Episode 193: total timesteps = 10, loss = 8.1022\n",
      "Episode 194: total timesteps = 10, loss = 7.8583\n",
      "Episode 195: total timesteps = 9, loss = 9.4151\n",
      "Episode 196: total timesteps = 9, loss = 11.4161\n",
      "Episode 197: total timesteps = 9, loss = 12.9496\n",
      "Episode 198: total timesteps = 10, loss = 7.7481\n",
      "Episode 199: total timesteps = 11, loss = 13.2803\n",
      "Episode 200: total timesteps = 9, loss = 9.2425\n",
      "Episode 201: total timesteps = 8, loss = 25.1576\n",
      "Episode 202: total timesteps = 9, loss = 11.0869\n",
      "Episode 203: total timesteps = 11, loss = 16.1170\n",
      "Episode 204: total timesteps = 10, loss = 8.3453\n",
      "Episode 205: total timesteps = 9, loss = 9.3939\n",
      "Episode 206: total timesteps = 10, loss = 10.1738\n",
      "Episode 207: total timesteps = 9, loss = 9.7250\n",
      "Episode 208: total timesteps = 9, loss = 7.7133\n",
      "Episode 209: total timesteps = 9, loss = 6.6890\n",
      "Episode 210: total timesteps = 10, loss = 12.1049\n",
      "Episode 211: total timesteps = 9, loss = 6.6873\n",
      "Episode 212: total timesteps = 10, loss = 14.7380\n",
      "Episode 213: total timesteps = 10, loss = 9.8909\n",
      "Episode 214: total timesteps = 10, loss = 9.9135\n",
      "Episode 215: total timesteps = 10, loss = 10.8493\n",
      "Episode 216: total timesteps = 9, loss = 6.8467\n",
      "Episode 217: total timesteps = 8, loss = 13.1234\n",
      "Episode 218: total timesteps = 9, loss = 8.8541\n",
      "Episode 219: total timesteps = 9, loss = 7.0768\n",
      "Episode 220: total timesteps = 9, loss = 7.6344\n",
      "Episode 221: total timesteps = 8, loss = 15.8570\n",
      "Episode 222: total timesteps = 10, loss = 8.8626\n",
      "Episode 223: total timesteps = 10, loss = 13.5943\n",
      "Episode 224: total timesteps = 9, loss = 6.5428\n",
      "Episode 225: total timesteps = 10, loss = 9.1843\n",
      "Episode 226: total timesteps = 10, loss = 8.2682\n",
      "Episode 227: total timesteps = 10, loss = 8.9355\n",
      "Episode 228: total timesteps = 9, loss = 6.5511\n",
      "Episode 229: total timesteps = 8, loss = 12.1911\n",
      "Episode 230: total timesteps = 10, loss = 9.0814\n",
      "Episode 231: total timesteps = 9, loss = 6.9513\n",
      "Episode 232: total timesteps = 10, loss = 9.2390\n",
      "Episode 233: total timesteps = 10, loss = 8.3987\n",
      "Episode 234: total timesteps = 9, loss = 7.8766\n",
      "Episode 235: total timesteps = 9, loss = 9.2907\n",
      "Episode 236: total timesteps = 10, loss = 8.1884\n",
      "Episode 237: total timesteps = 9, loss = 7.0606\n",
      "Episode 238: total timesteps = 9, loss = 10.6025\n",
      "Episode 239: total timesteps = 8, loss = 14.0880\n",
      "Episode 240: total timesteps = 9, loss = 8.0362\n",
      "Episode 241: total timesteps = 10, loss = 7.8540\n",
      "Episode 242: total timesteps = 8, loss = 16.1763\n",
      "Episode 243: total timesteps = 10, loss = 8.7879\n",
      "Episode 244: total timesteps = 10, loss = 15.7077\n",
      "Episode 245: total timesteps = 9, loss = 6.6215\n",
      "Episode 246: total timesteps = 9, loss = 6.6051\n",
      "Episode 247: total timesteps = 10, loss = 13.2692\n",
      "Episode 248: total timesteps = 9, loss = 7.0615\n",
      "Episode 249: total timesteps = 8, loss = 11.1778\n",
      "Episode 250: total timesteps = 8, loss = 11.5879\n",
      "Episode 251: total timesteps = 10, loss = 11.0724\n",
      "Episode 252: total timesteps = 8, loss = 10.4304\n",
      "Episode 253: total timesteps = 9, loss = 6.7859\n",
      "Episode 254: total timesteps = 10, loss = 14.9002\n",
      "Episode 255: total timesteps = 10, loss = 12.6270\n",
      "Episode 256: total timesteps = 8, loss = 8.6797\n",
      "Episode 257: total timesteps = 8, loss = 10.4007\n",
      "Episode 258: total timesteps = 9, loss = 6.5640\n",
      "Episode 259: total timesteps = 10, loss = 18.0823\n",
      "Episode 260: total timesteps = 10, loss = 12.6333\n",
      "Episode 261: total timesteps = 9, loss = 6.6920\n",
      "Episode 262: total timesteps = 10, loss = 12.4295\n",
      "Episode 263: total timesteps = 9, loss = 7.6969\n",
      "Episode 264: total timesteps = 10, loss = 9.8313\n",
      "Episode 265: total timesteps = 9, loss = 7.8316\n",
      "Episode 266: total timesteps = 10, loss = 8.5355\n",
      "Episode 267: total timesteps = 10, loss = 10.1430\n",
      "Episode 268: total timesteps = 8, loss = 18.6325\n",
      "Episode 269: total timesteps = 9, loss = 7.8463\n",
      "Episode 270: total timesteps = 9, loss = 10.3298\n",
      "Episode 271: total timesteps = 8, loss = 14.6976\n",
      "Episode 272: total timesteps = 9, loss = 7.1513\n",
      "Episode 273: total timesteps = 10, loss = 8.3332\n",
      "Episode 274: total timesteps = 9, loss = 8.3317\n",
      "Episode 275: total timesteps = 9, loss = 6.7438\n",
      "Episode 276: total timesteps = 9, loss = 6.5111\n",
      "Episode 277: total timesteps = 10, loss = 9.1859\n",
      "Episode 278: total timesteps = 10, loss = 8.3366\n",
      "Episode 279: total timesteps = 8, loss = 13.4399\n",
      "Episode 280: total timesteps = 10, loss = 14.6659\n",
      "Episode 281: total timesteps = 9, loss = 6.6460\n",
      "Episode 282: total timesteps = 9, loss = 6.4262\n",
      "Episode 283: total timesteps = 8, loss = 9.4577\n",
      "Episode 284: total timesteps = 8, loss = 10.0458\n",
      "Episode 285: total timesteps = 10, loss = 11.1793\n",
      "Episode 286: total timesteps = 8, loss = 7.9834\n",
      "Episode 287: total timesteps = 9, loss = 7.3621\n",
      "Episode 288: total timesteps = 10, loss = 16.4198\n",
      "Episode 289: total timesteps = 9, loss = 6.6606\n",
      "Episode 290: total timesteps = 9, loss = 6.4156\n",
      "Episode 291: total timesteps = 9, loss = 6.7271\n",
      "Episode 292: total timesteps = 9, loss = 7.5199\n",
      "Episode 293: total timesteps = 10, loss = 13.8362\n",
      "Episode 294: total timesteps = 9, loss = 6.4562\n",
      "Episode 295: total timesteps = 10, loss = 8.8831\n",
      "Episode 296: total timesteps = 10, loss = 7.6928\n",
      "Episode 297: total timesteps = 9, loss = 6.5021\n",
      "Episode 298: total timesteps = 10, loss = 10.1516\n",
      "Episode 299: total timesteps = 9, loss = 7.0137\n",
      "Episode 300: total timesteps = 10, loss = 9.0445\n",
      "Episode 301: total timesteps = 9, loss = 7.0095\n",
      "Episode 302: total timesteps = 9, loss = 10.3360\n",
      "Episode 303: total timesteps = 9, loss = 7.3963\n",
      "Episode 304: total timesteps = 10, loss = 7.4640\n",
      "Episode 305: total timesteps = 9, loss = 11.2006\n",
      "Episode 306: total timesteps = 8, loss = 19.0153\n",
      "Episode 307: total timesteps = 9, loss = 8.6022\n",
      "Episode 308: total timesteps = 10, loss = 7.5970\n",
      "Episode 309: total timesteps = 10, loss = 7.7421\n",
      "Episode 310: total timesteps = 10, loss = 7.7127\n",
      "Episode 311: total timesteps = 10, loss = 8.0752\n",
      "Episode 312: total timesteps = 9, loss = 7.3057\n",
      "Episode 313: total timesteps = 10, loss = 8.2049\n",
      "Episode 314: total timesteps = 10, loss = 11.9881\n",
      "Episode 315: total timesteps = 9, loss = 6.5059\n",
      "Episode 316: total timesteps = 10, loss = 14.1961\n",
      "Episode 317: total timesteps = 10, loss = 12.1025\n",
      "Episode 318: total timesteps = 8, loss = 14.1539\n",
      "Episode 319: total timesteps = 10, loss = 7.6721\n",
      "Episode 320: total timesteps = 8, loss = 16.0671\n",
      "Episode 321: total timesteps = 10, loss = 10.2006\n",
      "Episode 322: total timesteps = 10, loss = 8.8790\n",
      "Episode 323: total timesteps = 9, loss = 7.8360\n",
      "Episode 324: total timesteps = 10, loss = 7.8011\n",
      "Episode 325: total timesteps = 9, loss = 7.8996\n",
      "Episode 326: total timesteps = 9, loss = 7.6358\n",
      "Episode 327: total timesteps = 9, loss = 7.0703\n",
      "Episode 328: total timesteps = 9, loss = 6.7885\n",
      "Episode 329: total timesteps = 10, loss = 7.4499\n",
      "Episode 330: total timesteps = 9, loss = 6.6292\n",
      "Episode 331: total timesteps = 10, loss = 8.1150\n",
      "Episode 332: total timesteps = 10, loss = 10.9502\n",
      "Episode 333: total timesteps = 9, loss = 6.8160\n",
      "Episode 334: total timesteps = 8, loss = 10.2347\n",
      "Episode 335: total timesteps = 9, loss = 6.5329\n",
      "Episode 336: total timesteps = 9, loss = 6.7772\n",
      "Episode 337: total timesteps = 10, loss = 9.6262\n",
      "Episode 338: total timesteps = 9, loss = 6.4559\n",
      "Episode 339: total timesteps = 9, loss = 6.9555\n",
      "Episode 340: total timesteps = 9, loss = 7.1841\n",
      "Episode 341: total timesteps = 9, loss = 8.0284\n",
      "Episode 342: total timesteps = 10, loss = 8.2678\n",
      "Episode 343: total timesteps = 10, loss = 10.9586\n",
      "Episode 344: total timesteps = 8, loss = 12.7964\n",
      "Episode 345: total timesteps = 9, loss = 6.3563\n",
      "Episode 346: total timesteps = 9, loss = 6.3491\n",
      "Episode 347: total timesteps = 9, loss = 6.6929\n",
      "Episode 348: total timesteps = 8, loss = 9.7264\n",
      "Episode 349: total timesteps = 9, loss = 6.4132\n",
      "Episode 350: total timesteps = 9, loss = 6.4204\n",
      "Episode 351: total timesteps = 8, loss = 9.9954\n",
      "Episode 352: total timesteps = 8, loss = 9.6133\n",
      "Episode 353: total timesteps = 8, loss = 6.3315\n",
      "Episode 354: total timesteps = 10, loss = 19.3286\n",
      "Episode 355: total timesteps = 8, loss = 5.7596\n",
      "Episode 356: total timesteps = 10, loss = 20.7665\n",
      "Episode 357: total timesteps = 11, loss = 36.9533\n",
      "Episode 358: total timesteps = 8, loss = 6.9145\n",
      "Episode 359: total timesteps = 10, loss = 17.5714\n",
      "Episode 360: total timesteps = 8, loss = 6.6202\n",
      "Episode 361: total timesteps = 10, loss = 8.3236\n",
      "Episode 362: total timesteps = 8, loss = 10.1637\n",
      "Episode 363: total timesteps = 9, loss = 6.3400\n",
      "Episode 364: total timesteps = 10, loss = 8.9781\n",
      "Episode 365: total timesteps = 8, loss = 13.8551\n",
      "Episode 366: total timesteps = 10, loss = 7.9845\n",
      "Episode 367: total timesteps = 10, loss = 8.8030\n",
      "Episode 368: total timesteps = 8, loss = 19.3411\n",
      "Episode 369: total timesteps = 9, loss = 7.6242\n",
      "Episode 370: total timesteps = 9, loss = 7.2319\n",
      "Episode 371: total timesteps = 9, loss = 8.4882\n",
      "Episode 372: total timesteps = 8, loss = 13.3072\n",
      "Episode 373: total timesteps = 10, loss = 9.8693\n",
      "Episode 374: total timesteps = 10, loss = 14.8874\n",
      "Episode 375: total timesteps = 10, loss = 13.3899\n",
      "Episode 376: total timesteps = 10, loss = 13.0381\n",
      "Episode 377: total timesteps = 9, loss = 6.3456\n",
      "Episode 378: total timesteps = 10, loss = 8.4591\n",
      "Episode 379: total timesteps = 9, loss = 6.3927\n",
      "Episode 380: total timesteps = 10, loss = 8.1936\n",
      "Episode 381: total timesteps = 8, loss = 12.1144\n",
      "Episode 382: total timesteps = 10, loss = 8.0819\n",
      "Episode 383: total timesteps = 10, loss = 7.3839\n",
      "Episode 384: total timesteps = 10, loss = 7.5431\n",
      "Episode 385: total timesteps = 9, loss = 6.9449\n",
      "Episode 386: total timesteps = 10, loss = 7.2216\n",
      "Episode 387: total timesteps = 10, loss = 7.8237\n",
      "Episode 388: total timesteps = 10, loss = 7.3608\n",
      "Episode 389: total timesteps = 10, loss = 7.6293\n",
      "Episode 390: total timesteps = 9, loss = 7.7558\n",
      "Episode 391: total timesteps = 8, loss = 16.0326\n",
      "Episode 392: total timesteps = 11, loss = 11.9789\n",
      "Episode 393: total timesteps = 10, loss = 8.0417\n",
      "Episode 394: total timesteps = 10, loss = 7.3702\n",
      "Episode 395: total timesteps = 8, loss = 13.1875\n",
      "Episode 396: total timesteps = 10, loss = 7.1859\n",
      "Episode 397: total timesteps = 10, loss = 8.4967\n",
      "Episode 398: total timesteps = 10, loss = 7.4772\n",
      "Episode 399: total timesteps = 9, loss = 6.9539\n",
      "Episode 400: total timesteps = 9, loss = 7.9393\n",
      "Episode 401: total timesteps = 9, loss = 7.1598\n",
      "Episode 402: total timesteps = 8, loss = 16.4651\n",
      "Episode 403: total timesteps = 10, loss = 9.8501\n",
      "Episode 404: total timesteps = 9, loss = 6.7641\n",
      "Episode 405: total timesteps = 8, loss = 12.7755\n",
      "Episode 406: total timesteps = 9, loss = 6.5472\n",
      "Episode 407: total timesteps = 10, loss = 12.1570\n",
      "Episode 408: total timesteps = 9, loss = 6.5935\n",
      "Episode 409: total timesteps = 8, loss = 8.2325\n",
      "Episode 410: total timesteps = 10, loss = 16.7996\n",
      "Episode 411: total timesteps = 10, loss = 12.3217\n",
      "Episode 412: total timesteps = 10, loss = 15.6721\n",
      "Episode 413: total timesteps = 10, loss = 10.4948\n",
      "Episode 414: total timesteps = 8, loss = 11.7730\n",
      "Episode 415: total timesteps = 10, loss = 9.6155\n",
      "Episode 416: total timesteps = 9, loss = 6.4658\n",
      "Episode 417: total timesteps = 9, loss = 6.5538\n",
      "Episode 418: total timesteps = 10, loss = 9.8599\n",
      "Episode 419: total timesteps = 9, loss = 6.5621\n",
      "Episode 420: total timesteps = 8, loss = 17.8401\n",
      "Episode 421: total timesteps = 9, loss = 7.6993\n",
      "Episode 422: total timesteps = 9, loss = 9.4570\n",
      "Episode 423: total timesteps = 9, loss = 6.3986\n",
      "Episode 424: total timesteps = 9, loss = 6.6532\n",
      "Episode 425: total timesteps = 8, loss = 10.0933\n",
      "Episode 426: total timesteps = 10, loss = 11.2209\n",
      "Episode 427: total timesteps = 10, loss = 8.7610\n",
      "Episode 428: total timesteps = 9, loss = 6.4726\n",
      "Episode 429: total timesteps = 10, loss = 12.0393\n",
      "Episode 430: total timesteps = 9, loss = 6.3990\n",
      "Episode 431: total timesteps = 9, loss = 6.3353\n",
      "Episode 432: total timesteps = 10, loss = 10.6957\n",
      "Episode 433: total timesteps = 8, loss = 15.2653\n",
      "Episode 434: total timesteps = 8, loss = 12.3678\n",
      "Episode 435: total timesteps = 9, loss = 6.7343\n",
      "Episode 436: total timesteps = 10, loss = 9.4382\n",
      "Episode 437: total timesteps = 9, loss = 6.3556\n",
      "Episode 438: total timesteps = 9, loss = 6.6212\n",
      "Episode 439: total timesteps = 10, loss = 12.4735\n",
      "Episode 440: total timesteps = 8, loss = 7.0700\n",
      "Episode 441: total timesteps = 10, loss = 7.8477\n",
      "Episode 442: total timesteps = 8, loss = 13.0864\n",
      "Episode 443: total timesteps = 9, loss = 6.6542\n",
      "Episode 444: total timesteps = 9, loss = 6.4199\n",
      "Episode 445: total timesteps = 10, loss = 11.4374\n",
      "Episode 446: total timesteps = 10, loss = 10.0000\n",
      "Episode 447: total timesteps = 10, loss = 13.7454\n",
      "Episode 448: total timesteps = 9, loss = 6.7622\n",
      "Episode 449: total timesteps = 8, loss = 12.6370\n",
      "Episode 450: total timesteps = 9, loss = 7.0017\n",
      "Episode 451: total timesteps = 10, loss = 8.6130\n",
      "Episode 452: total timesteps = 10, loss = 7.6761\n",
      "Episode 453: total timesteps = 10, loss = 7.4960\n",
      "Episode 454: total timesteps = 9, loss = 6.5719\n",
      "Episode 455: total timesteps = 9, loss = 7.9051\n",
      "Episode 456: total timesteps = 9, loss = 7.8282\n",
      "Episode 457: total timesteps = 9, loss = 8.9858\n",
      "Episode 458: total timesteps = 9, loss = 6.7373\n",
      "Episode 459: total timesteps = 10, loss = 7.2109\n",
      "Episode 460: total timesteps = 9, loss = 6.6345\n",
      "Episode 461: total timesteps = 10, loss = 11.3931\n",
      "Episode 462: total timesteps = 10, loss = 11.6756\n",
      "Episode 463: total timesteps = 10, loss = 11.6672\n",
      "Episode 464: total timesteps = 8, loss = 9.4772\n",
      "Episode 465: total timesteps = 10, loss = 7.3993\n",
      "Episode 466: total timesteps = 8, loss = 11.1459\n",
      "Episode 467: total timesteps = 8, loss = 14.9536\n",
      "Episode 468: total timesteps = 10, loss = 8.8450\n",
      "Episode 469: total timesteps = 8, loss = 10.2207\n",
      "Episode 470: total timesteps = 9, loss = 6.9467\n",
      "Episode 471: total timesteps = 10, loss = 12.7077\n",
      "Episode 472: total timesteps = 10, loss = 7.3835\n",
      "Episode 473: total timesteps = 10, loss = 9.6896\n",
      "Episode 474: total timesteps = 10, loss = 7.5203\n",
      "Episode 475: total timesteps = 11, loss = 15.5392\n",
      "Episode 476: total timesteps = 9, loss = 7.4080\n",
      "Episode 477: total timesteps = 10, loss = 7.5340\n",
      "Episode 478: total timesteps = 11, loss = 14.6381\n",
      "Episode 479: total timesteps = 10, loss = 7.8641\n",
      "Episode 480: total timesteps = 9, loss = 7.2822\n",
      "Episode 481: total timesteps = 9, loss = 13.8882\n",
      "Episode 482: total timesteps = 10, loss = 7.1857\n",
      "Episode 483: total timesteps = 9, loss = 8.7496\n",
      "Episode 484: total timesteps = 9, loss = 10.1611\n",
      "Episode 485: total timesteps = 10, loss = 7.5264\n",
      "Episode 486: total timesteps = 10, loss = 7.2153\n",
      "Episode 487: total timesteps = 10, loss = 8.6639\n",
      "Episode 488: total timesteps = 8, loss = 16.5027\n",
      "Episode 489: total timesteps = 8, loss = 9.6790\n",
      "Episode 490: total timesteps = 9, loss = 6.5260\n",
      "Episode 491: total timesteps = 9, loss = 7.7542\n",
      "Episode 492: total timesteps = 9, loss = 6.5673\n",
      "Episode 493: total timesteps = 10, loss = 15.4981\n",
      "Episode 494: total timesteps = 10, loss = 8.4169\n",
      "Episode 495: total timesteps = 9, loss = 6.6211\n",
      "Episode 496: total timesteps = 9, loss = 6.5464\n",
      "Episode 497: total timesteps = 10, loss = 13.7875\n",
      "Episode 498: total timesteps = 10, loss = 8.4392\n",
      "Episode 499: total timesteps = 9, loss = 6.5724\n",
      "Episode 500: total timesteps = 9, loss = 6.5308\n"
     ]
    }
   ],
   "source": [
    "# Run a few training episodes\n",
    "run_episode_and_train(num_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e77dc",
   "metadata": {
    "_cell_guid": "ba65e3fd-7d32-4547-a0d6-d25c6adc470a",
    "_uuid": "9a3880c8-9469-470b-b78d-942b3d0cddec",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020592,
     "end_time": "2025-06-30T11:39:04.646627",
     "exception": false,
     "start_time": "2025-06-30T11:39:04.626035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results and Evaluation\n",
    "\n",
    "Once trained, we can evaluate the MuZero agent by running it (with search) on CartPole. We expect it to achieve high episode lengths (ideally 500 consistently). Below we test the agent’s performance after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf48ad2",
   "metadata": {
    "_cell_guid": "fc608c62-65d4-4192-b830-609478bc1503",
    "_uuid": "9131cb2e-e112-4333-b6ef-f6cbcd56a3f5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-30T11:39:04.688825Z",
     "iopub.status.busy": "2025-06-30T11:39:04.688586Z",
     "iopub.status.idle": "2025-06-30T11:39:07.984146Z",
     "shell.execute_reply": "2025-06-30T11:39:07.983333Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.318362,
     "end_time": "2025-06-30T11:39:07.985583",
     "exception": false,
     "start_time": "2025-06-30T11:39:04.667221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 1: length = 8.0\n",
      "Evaluation Episode 2: length = 10.0\n",
      "Evaluation Episode 3: length = 11.0\n",
      "Evaluation Episode 4: length = 10.0\n",
      "Evaluation Episode 5: length = 9.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent\n",
    "num_eval_episodes = 5\n",
    "for i in range(num_eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # MCTS to choose action\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            hidden2 = rep_net(obs_tensor)  # use rep as hidden2\n",
    "        root = MCTSNode(hidden2, parent=None)\n",
    "        search_pi = run_mcts(root)\n",
    "        action = int(np.argmax(search_pi))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    print(f\"Evaluation Episode {i+1}: length = {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae648f",
   "metadata": {
    "_cell_guid": "52080126-20fd-49d2-a282-8caff280e41f",
    "_uuid": "0474e822-4334-4738-b9cf-9cb2b4e21fba",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0225,
     "end_time": "2025-06-30T11:39:08.030056",
     "exception": false,
     "start_time": "2025-06-30T11:39:08.007556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have built MuZero from first principles for the CartPole control problem. We explained how MuZero learns a latent model (value, policy, reward) without an explicit simulator, and how it uses Monte Carlo Tree Search to plan. Our implementation achieved a reproducible agent that learns to balance the pole by integrating neural network function approximation with algorithmic search.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "* CartPole is a 4-dimensional continuous MDP with sparse rewards.\n",
    "* The Bellman optimality equation guides value estimation in MDPs.\n",
    "* MuZero builds a model that predicts rewards, values, and policies directly.\n",
    "* Monte Carlo Tree Search uses the learned model to explore future actions.\n",
    "* Training enforces consistency between network predictions and search-enhanced targets (policy/value) along with actual rewards.\n",
    "\n",
    "This approach yields an agent that learns from and plans for the CartPole environment, illustrating the power of combining symbolic planning (search) and subsymbolic learning (neural networks).\n",
    "\n",
    "**References:** The above implementation and derivations are based on MuZero theory and code, drawing on DeepMind’s original MuZero paper, the DeepMind blog, and OpenAI Gym’s CartPole documentation, among others. These sources provide the foundational concepts for model-based RL with planning."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 354.364473,
   "end_time": "2025-06-30T11:39:10.072773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-30T11:33:15.708300",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
